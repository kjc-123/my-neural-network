# Neural Network Implementation
Neural network with one hidden layer training on MNIST

Done out of my personal curiosity

warning - very scuffed; issues include:
 - dimensions of everything are backwards from what is standard
 - only implemented one (1) hidden layer with sixteen (16) neurons; code is not very generalizable in present state
 - many files with useless/broken tests

Special thanks goes to 3blue1brown for his series on Neural Networks, which is how I initially learned everything, and Michael Nielsen, whose neural network implementation I referenced.

3blue1brown's neural network series: https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=YxSXqAlSRpeBfm8a

Github repo for Michael Nielsen's NN implementation: https://github.com/mnielsen/neural-networks-and-deep-learning

TODO:
- [ ] Generalize number of hidden layers
- [ ] Generalize neurons in each layer
- [ ] Clean up tests
- [ ] Correctly implement batch processing
- [ ] Implement softmax
- [ ] Implement cross-entropy
- [ ] Implement initialization methods
- [ ] Generalize problem